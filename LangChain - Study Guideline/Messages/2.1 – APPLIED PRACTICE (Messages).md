| **Basic Examples**                    | **Intermediate Examples**               | **Advanced Examples**                                   |
| ------------------------------------- | --------------------------------------- | ------------------------------------------------------- |
| Basic Message Construction            | Multi-Turn Message Conversation         | Full Tool Loop Agent (AI → Tool → AI)                   |
| Simple Chat Model Call (Messages)     | Memory-Augmented Message Chat           | Message-Orchestrated Workflow Pipeline                  |
| Minimal LCEL with Messages            | Message-Based Tool Calling              | Multi-Step Reasoning with Message History Management    |
| Single-Turn Conversation with History | Conditional / Branching Message Routing | Hybrid Message System (Content Blocks + Memory + Tools) |

---
## **1. Basic Message Construction**

Shows how to create System, Human, and AI messages using the updated LangChain message API. Builds foundational understanding of role-based communication.

```python
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="What is LangChain?")
]

# Simulate an AI response in local testing
messages.append(AIMessage(content="LangChain is a framework for LLM orchestration."))
print(messages)
```

---

## **2. Simple Chat Model Call (Messages)**

The simplest way to call a chat model using a message array. Demonstrates HumanMessage → AIMessage flow.

```python
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage

llm = ChatGroq(model="llama-3.1-8b-instant")

response = llm.invoke([
    HumanMessage(content="Explain 'messages' in LangChain in one sentence.")
])

print(response.content)
```

---

## **3. Minimal LCEL With Messages**

Connects message templates directly into an LLM using LCEL’s pipeline operator. Perfect for compact, message-driven workflows.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a poetic assistant."),
    ("human", "Write a haiku about {topic}.")
])

llm = ChatGroq(model="llama-3.1-8b-instant")

chain = prompt | llm
response = chain.invoke({"topic": "messages in LangChain"})
print(response.content)
```

---

## **4. Single-Turn Conversation with History**

Shows how adding previous Human+AI messages shapes current context — messages _are_ the conversation state.

```python
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage, AIMessage

llm = ChatGroq(model="llama-3.1-8b-instant")

history = [
    HumanMessage(content="Hello, who are you?"),
    AIMessage(content="I am an AI assistant.")
]

history.append(HumanMessage(content="What did I just say?"))
response = llm.invoke(history)

print(response.content)
```

---

# **INTERMEDIATE EXAMPLES (4)**

---

## **5. Multi-Turn Message Conversation**

Demonstrates an evolving message history across turns — essential for chatbots and persistent agents.

```python
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage, AIMessage

llm = ChatGroq(model="llama-3.1-8b-instant")

history = []
history.append(HumanMessage(content="My name is Alex."))
history.append(llm.invoke(history))

history.append(HumanMessage(content="What is my name?"))
response = llm.invoke(history)

print(response.content)
```

---

## **6. Memory-Augmented Message Chat**

Uses Summary Memory to compress long message histories into concise summaries for scalable conversations.

```python
from langchain.memory import ConversationSummaryMemory
from langchain.chains import ConversationChain
from langchain_groq import ChatGroq

llm = ChatGroq(model="llama-3.1-8b-instant")
memory = ConversationSummaryMemory(llm=llm)

chain = ConversationChain(llm=llm, memory=memory)

chain.run("I love space exploration.")
print(chain.run("What topic did I say I love?"))
```

---

## **7. Message-Based Tool Calling**

Shows how AIMessage can trigger a tool using structured `tool_calls` — the core of agentic behavior.

```python
from langchain_core.tools import tool
from langchain_groq import ChatGroq
from langchain.agents import create_tool_calling_agent, AgentExecutor

@tool
def add(a: int, b: int) -> int:
    return a + b

llm = ChatGroq(model="llama-3.1-8b-instant")

agent = create_tool_calling_agent(llm, [add])
executor = AgentExecutor(agent=agent, tools=[add], verbose=True)

executor.invoke({"input": "Add 20 and 22."})
```

---

## **8. Conditional / Branching Message Routing**

Routes messages differently depending on content — useful for decision trees, triage systems, or multi-mode assistants.

```python
from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableBranch
from langchain_groq import ChatGroq

llm = ChatGroq(model="llama-3.1-8b-instant")

math_chain = lambda x: llm.invoke([HumanMessage(content=f"Solve: {x['input']}")])
general_chain = lambda x: llm.invoke([HumanMessage(content=f"Answer: {x['input']}")])

branch = RunnableBranch(
    (lambda x: any(op in x["input"] for op in ["+", "-", "*", "/"]), math_chain),
    general_chain
)

print(branch.invoke({"input": "12 * 12"}).content)
```

---

# **ADVANCED EXAMPLES (4)**

---

## **9. Full Tool Loop Agent (AI → Tool → AI)**

A complete demonstration of LangChain’s message-based tool loop system.

```python
from langchain_core.tools import tool
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_groq import ChatGroq

@tool
def multiply(a: int, b: int) -> int:
    return a * b

llm = ChatGroq(model="llama-3.1-70b-versatile")

agent = create_tool_calling_agent(llm, [multiply])
executor = AgentExecutor(agent=agent, tools=[multiply], verbose=True)

executor.invoke({"input": "Multiply 23 by 45."})
```

---

## **10. Message-Orchestrated Workflow Pipeline**

Chains messages, prompts, and transforms using LCEL — a stepping stone to production agents.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq

prompt = ChatPromptTemplate.from_messages([
    ("human", "Rewrite this more clearly:\n{input}")
])

llm = ChatGroq(model="llama-3.1-8b-instant")
chain = prompt | llm | StrOutputParser()

print(chain.invoke({"input": "LangChain messages help structure LLM input."}))
```

---

## **11. Multi-Step Reasoning with Message History Management**

Uses sliding window message trimming to prevent context overload.

```python
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_groq import ChatGroq

llm = ChatGroq(model="llama-3.1-70b-versatile")

messages = [
    SystemMessage(content="You are a reasoning assistant.")
]

for i in range(20):
    messages.append(HumanMessage(content=f"Step {i}: continue reasoning."))

# Trim to last N messages manually or using utilities
messages = messages[-8:]

print(llm.invoke(messages).content)
```

---

## **12. Hybrid Message System (Content Blocks + Memory + Tools)**

Combines multimodal content, memory, and tool loops to build a production-like agent.

```python
from langchain_core.messages import HumanMessage
from langchain_core.messages import ImageContentBlock
from langchain_groq import ChatGroq
from langchain.memory import ConversationBufferMemory
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.tools import tool

@tool
def identify_color(name: str) -> str:
    return f"The color {name} is vibrant."

llm = ChatGroq(model="llama-3.1-70b-versatile")
memory = ConversationBufferMemory()

image = ImageContentBlock(image_url="https://example.com/sample.jpg")

human_msg = HumanMessage(content=[image, "Describe the main colors."])

agent = create_tool_calling_agent(llm, [identify_color])
executor = AgentExecutor(agent=agent, tools=[identify_color], verbose=True)

history = memory.load_memory_variables({})
result = executor.invoke({"input": human_msg})
memory.save_context({"input": human_msg}, {"output": result})

print(result)
```

---

If you want, I can also generate:

✅ **2.2 Mini Projects (Messages)**  
or  
✅ **2.3 Patterns & Workflows (Messages)**  
or  
✅ convert this whole section into Obsidian-style pages with your backlink format.

Just tell me!