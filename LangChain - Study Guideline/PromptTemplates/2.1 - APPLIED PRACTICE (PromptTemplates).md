Great — we will now print each code example one by one, following the order from your 3-column table:

| Basic Examples                       | Intermediate Examples          | Advanced Examples                            |
| ------------------------------------ | ------------------------------ | -------------------------------------------- |
| Basic PromptTemplate Usage           | Mini RAG: Retriever + LLM      | Multi-Step Agent with Tools                  |
| Simple Chat Model Call               | Memory-Enhanced Chatbot        | Full RAG Pipeline (Compression + Re-ranking) |
| Minimal LCEL Chain                   | Tool Calling Example           | Workflow Chain with Validation & Fallback    |
| Single-Turn Conversation with Memory | Parallel / Branching LCEL Flow | Hybrid Memory System                         |

---

**BASIC EXAMPLES (4)**

1. **Basic PromptTemplate Usage**  
Shows how PromptTemplate turns dynamic user input into consistent, reproducible prompts. This example teaches variable injection, template formatting, and the foundation of reusable prompt engineering.

```python
from langchain_core.prompts import PromptTemplate
from langchain_groq import ChatGroq

llm = ChatGroq(model="llama-3.1-8b-instant", temperature=0)

template = PromptTemplate.from_template(
    "Translate the following text to French:\n{text}"
)

chain = template | llm
response = chain.invoke({"text": "Hello, world!"})
print(response.content)
```

2. **Simple Chat Model Call**  
A single direct message sent to a chat model. This demonstrates how to structure human messages, call the model, and print the generated response. It is the simplest usable LLM invocation pattern.

```python
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage

llm = ChatGroq(model="llama-3.1-8b-instant")
response = llm.invoke([HumanMessage(content="Explain LangChain in one sentence.")])
print(response.content)
```

3. **Minimal LCEL Chain**  
Introduces the LCEL pipeline operator (|), connecting a prompt directly to a model. This compact pattern is the foundation of all multi-step, composable workflows and advanced chains.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq

prompt = ChatPromptTemplate.from_messages([
    ("human", "Write a haiku about {topic}.")
])

llm = ChatGroq(model="llama-3.1-8b-instant")
chain = prompt | llm

response = chain.invoke({"topic": "AI systems"})
print(response.content)
```

4. **Single-Turn Conversation with Memory**  
Shows how memory preserves context across calls. This example uses buffer memory to store conversation history, enabling context-aware responses across multiple turns of interaction.

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain_groq import ChatGroq

llm = ChatGroq(model="llama-3.1-8b-instant")
memory = ConversationBufferMemory()

chain = ConversationChain(llm=llm, memory=memory)

print(chain.run("Hello, who are you?"))
print(chain.run("What did I just ask you?"))
```

**INTERMEDIATE EXAMPLES (4)**

5. **Mini RAG: Retriever + LLM**  
A minimal Retrieval-Augmented Generation example. Loads text, embeds it, stores it in a vectorstore, retrieves relevant chunks, and passes them to the LLM as contextual grounding.

```python
from langchain_community.document_loaders import TextLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

docs = TextLoader("sample.txt").load()
vectorstore = FAISS.from_documents(docs, HuggingFaceEmbeddings())
retriever = vectorstore.as_retriever()

llm = ChatGroq(model="llama-3.1-8b-instant")

template = """Answer based only on this context:\n{context}\n\nQuestion: {question}"""
prompt = ChatPromptTemplate.from_template(template)

chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt | llm | StrOutputParser()
)

print(chain.invoke("What is the main topic of the document?"))
```

6. **Memory-Enhanced Chatbot**  
Uses Summary Memory to maintain conversation meaning over long dialogues. The memory condenses past messages, producing efficient, scalable multi-turn chat.

```python
from langchain.memory import ConversationSummaryMemory
from langchain.chains import ConversationChain
from langchain_groq import ChatGroq

llm = ChatGroq(model="llama-3.1-8b-instant")
memory = ConversationSummaryMemory(llm=llm)

chain = ConversationChain(llm=llm, memory=memory)

chain.run("My name is Alex and I work in AI.")
print(chain.run("What did I say my name was and what do I do?"))
```

7. **Tool Calling Example**  
Demonstrates structured tool execution. The model outputs a JSON tool-call request → Python executes the function → result feeds back to the model. Foundation of agent capabilities.

```python
from langchain_core.tools import tool
from langchain_groq import ChatGroq
from langchain import hub
from langchain.agents import create_tool_calling_agent, AgentExecutor

@tool
def multiply(a: int, b: int) -> int:
    """Multiply two integers."""
    return a * b

llm = ChatGroq(model="llama-3.1-70b-versatile")
prompt = hub.pull("hwchase17/openai-functions-agent")

agent = create_tool_calling_agent(llm, [multiply], prompt)
executor = AgentExecutor(agent=agent, tools=[multiply], verbose=True)

executor.invoke({"input": "What is 23 multiplied by 45?"})
```

8. **Parallel / Branching LCEL Flow**  
Demonstrates conditional and parallel logic in LCEL. Useful for workflows where decisions or multiple simultaneous computations are required.

```python
from langchain_core.runnables import RunnableParallel, RunnableBranch
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq

llm = ChatGroq(model="llama-3.1-8b-instant")

math = ChatPromptTemplate.from_template("Solve this math problem: {input}") | llm
general = ChatPromptTemplate.from_template("Answer: {input}") | llm

branch = RunnableBranch(
    (lambda x: "math" in x["input"].lower(), math),
    (lambda x: "code" in x["input"].lower(), general),
    general,
)

parallel = RunnableParallel({"branch": branch, "original": lambda x: x["input"]})
print(parallel.invoke({"input": "What is 12 * 12?"}))
```

**ADVANCED EXAMPLES (4)**

9. **Multi-Step Agent with Tools**  
A ReAct-style agent capable of planning actions, calling external tools, observing results, and iterating toward a final response. This mirrors real-world agentic behavior.

```python
from langchain import hub
from langchain.agents import create_react_agent, AgentExecutor
from langchain.tools import DuckDuckGoSearchRun
from langchain_groq import ChatGroq

llm = ChatGroq(model="llama-3.1-70b-versatile")
tools = [DuckDuckGoSearchRun()]
prompt = hub.pull("hwchase17/react")

agent = create_react_agent(llm, tools, prompt)
executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

executor.invoke({"input": "What are the latest breakthroughs in Groq as of November 2025?"})
```

10. **Full RAG Pipeline (Compression + Re-ranking)**  
A high-quality RAG example that improves retrieval accuracy using LLM-based compression, filtering, and semantic refinement.

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# assuming `retriever` from example 5 exists
llm = ChatGroq(model="llama-3.1-70b-versatile")
compressor = LLMChainExtractor.from_llm(llm)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=retriever
)

template = "Answer based only on this context:\n{context}\n\nQuestion: {question}"
chain = (
    {"context": compression_retriever, "question": lambda x: x}
    | ChatPromptTemplate.from_template(template)
    | llm
    | StrOutputParser()
)

print(chain.invoke("Summarize the core idea in one sentence."))
```

11. **Workflow Chain with Validation & Fallback**  
A production pattern using schema validation, retry logic, and a backup model to guarantee structured output even when the primary model fails.

```python
from langchain_groq import ChatGroq
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel, Field

class Joke(BaseModel):
    setup: str = Field(description="The setup of the joke")
    punchline: str = Field(description="The punchline")

parser = PydanticOutputParser(pydantic_object=Joke)

llm_fast = ChatGroq(model="llama-3.1-8b-instant")
llm_strong = ChatGroq(model="llama-3.1-70b-versatile")

prompt = PromptTemplate.from_template(
    "Tell a joke about AI.\n{format_instructions}"
).partial(format_instructions=parser.get_format_instructions())

chain = prompt | llm_fast | parser
fallback_chain = prompt | llm_strong | parser

try:
    result = chain.invoke({})
except:
    result = fallback_chain.invoke({})

print(result)
```

12. **Hybrid Memory System**  
Combines multiple memory types (buffer + summary + token) for long-context dialogue, mirroring real production agents managing large interactions.

```python
from langchain.memory import CombinedMemory, ConversationBufferMemory, ConversationSummaryBufferMemory
from langchain_groq import ChatGroq
from langchain.chains import ConversationChain

llm = ChatGroq(model="llama-3.1-70b-versatile")

memory = CombinedMemory(memories=[
    ConversationBufferMemory(),
    ConversationSummaryBufferMemory(llm=llm, max_token_limit=1000)
])

chain = ConversationChain(llm=llm, memory=memory, verbose=True)

print(chain.run("Tell me a very long story about space exploration..."))
print(chain.run("Now summarize everything we just talked about."))
```