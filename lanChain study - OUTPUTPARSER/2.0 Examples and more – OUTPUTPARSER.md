---
aliases: 
  - "OutputParser Code Examples"
  - "Groq + LangChain Examples"
  - "Structured Output Snippets"
  - "Pydantic Parser Examples"
  - "Production Ready Chains"
tags:
  - llm-engineering
  - code-examples
  - groq
  - langchain
  - pydantic
  - outputparser
  - structured-output
  - mini-project
  - beginner-to-advanced
  - 2025
status: complete
difficulty: intermediate
importance: 10/10
mastery: 95%
created: 2025-11-24
updated: 2025-11-24
review-date: 2025-12-31
type: code-reference
layer: applied-practice
related:
  - "[[Five Core LLM Design Patterns â€” Production 2025]]"
  - "[[Canonical LLM Workflows â€” Production 2025]]"
  - "[[analyzer.py â€” Customer Message Analyzer]]"
  - "[[OutputParser â€” Foundations]]"
  - "[[Groq]]"
  - "[[LangChain]]"
cssclass: clean-embed, code-heavy
banner: "https://i.imgur.com/5kL3vZm.png"  # clean terminal/code aesthetic
banner_y: 0.62
icon: ğŸ’»
color: green
---
## 2.1.1 BASIC EXAMPLE â€” Extract Structured Data (Groq + LangChain)

```python
"""
Basic Example: Structured JSON extraction using Groq + LangChain OutputParser.
"""

from groq import Groq
from langchain.output_parsers import JsonOutputParser
from langchain.prompts import PromptTemplate

client = Groq(api_key="YOUR_GROQ_API_KEY")

parser = JsonOutputParser()

prompt = PromptTemplate(
    template="""
Return a JSON object describing the sentiment of this text:

Text: "{text}"

{format_instructions}
""",
    input_variables=["text"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)

chain = prompt | (lambda x: client.chat.completions.create(
    model="llama3-8b-8192",
    messages=[{"role": "user", "content": x}]
).choices[0].message.content) | parser

result = chain.invoke({"text": "i love this app but it crashes sometimes"})

print(result)
```
## 2.1.2 INTERMEDIATE EXAMPLE â€” Pydantic Schema + Auto-Validation

```python
"""
Intermediate Example: Validate model output using PydanticOutputParser.
"""

from groq import Groq
from pydantic import BaseModel, Field
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate

client = Groq(api_key="YOUR_GROQ_API_KEY")

class SupportAnalysis(BaseModel):
    sentiment: str = Field(..., description="positive, neutral, or negative")
    issues: list[str]
    needs_refund: bool

parser = PydanticOutputParser(pydantic_object=SupportAnalysis)

prompt = PromptTemplate(
    template="""
Analyze the customer message below and return JSON that matches this schema:

{format_instructions}

Message: "{message}"
""",
    input_variables=["message"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)

def call_llm(prompt_text):
    response = client.chat.completions.create(
        model="llama3-8b-8192",
        messages=[{"role": "user", "content": prompt_text}]
    )
    return response.choices[0].message.content

chain = prompt | call_llm | parser

result = chain.invoke({"message": "my iphone app keeps freezing and i'm angry"})

print(result)
```

## 2.1.3 ADVANCED EXAMPLE â€” Full Production Chain (Retry, Logging, Strict JSON)

```python
"""
Advanced Example: Production-grade chain with retries.
"""

from groq import Groq
from pydantic import BaseModel, Field
from langchain.output_parsers import PydanticOutputParser
from langchain.output_parsers.retry import RetryOutputParser
from langchain.prompts import PromptTemplate

client = Groq(api_key="YOUR_GROQ_API_KEY")

class Review(BaseModel):
    rating: int = Field(..., ge=1, le=5)
    summary: str
    keywords: list[str]

base_parser = PydanticOutputParser(pydantic_object=Review)
retry_parser = RetryOutputParser.from_llm(parser=base_parser, llm=None)  # Groq fallback via callback

prompt = PromptTemplate(
    template="""
Summarize this product review using the schema below.

{format_instructions}

Review: "{review}"
""",
    input_variables=["review"],
    partial_variables={"format_instructions": base_parser.get_format_instructions()},
)

def call_llm(text):
    response = client.chat.completions.create(
        model="mixtral-8x7b-32768",
        messages=[{"role": "user", "content": text}]
    )
    return response.choices[0].message.content

chain = prompt | call_llm | retry_parser

result = chain.invoke({"review": "The laptop is fast but gets hot and the battery is weak."})

print(result)
```


# **ğŸ”¥ analyzer.py â€” FULL MINI-PROJECT**

```python
"""
Mini Project: Customer Message Analyzer (Beginner Project)
----------------------------------------------------------

This single-file script:
1. Sends a user complaint message to the LLM
2. Parses structured JSON using LangChain's PydanticOutputParser
3. Auto-validates types (int, bool, list, enums)
4. Produces safe, typed output every time

Requirements:
    pip install langchain pydantic langchain-groq python-dotenv

Environment:
    Add GROQ_API_KEY to your environment
"""

from langchain_groq import ChatGroq
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from pydantic import BaseModel, Field


# ---------------------------------------------------
# 1. Define the schema (the contract LLM must follow)
# ---------------------------------------------------

class MessageAnalysis(BaseModel):
    sentiment: str = Field(..., description="positive, neutral, or negative")
    intensity: int = Field(..., ge=1, le=5, description="severity 1â€“5")
    products: list[str]
    issues: list[str]
    needs_human: bool


parser = PydanticOutputParser(pydantic_object=MessageAnalysis)


# ---------------------------------------------------
# 2. Prompt: Asks the LLM to follow the schema
# ---------------------------------------------------

prompt = PromptTemplate(
    template="""
Analyze the following customer message and return ONLY valid JSON.

Return EXACTLY in this format:
{format_instructions}

Customer Message:
{user_text}
""",
    input_variables=["user_text"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)


# ---------------------------------------------------
# 3. Load a FREE Groq model (fast + reliable)
# ---------------------------------------------------

llm = ChatGroq(
    model="mixtral-8x7b-instruct",  # free + high quality
    temperature=0
)


# ---------------------------------------------------
# 4. Build the chain
# ---------------------------------------------------

chain = prompt | llm | parser


# ---------------------------------------------------
# 5. Example input (simulate a complaint)
# ---------------------------------------------------

user_message = """
your app keeps freezing on my samsung phone
and now the login doesn't work after the update.
this is super annoying fix it asap
"""


# ---------------------------------------------------
# 6. Run the chain end-to-end
# ---------------------------------------------------

result: MessageAnalysis = chain.invoke({"user_text": user_message})


# ---------------------------------------------------
# 7. Use the structured output
# ---------------------------------------------------

print("\n========== ANALYSIS RESULT ==========")
print(result)
print("\nAs dict:", result.dict())

print("\n========== ROUTING DECISION ==========")

if result.intensity >= 4 or result.needs_human:
    print("âš ï¸ High severity â€” escalate to human support.")
else:
    print("ğŸ‘Œ AI auto-response is sufficient.")
```

---
# 2.3 Patterns & Workflows

### 2.3.1 Core Design Patterns â€” Fully Extended

**1. PMP (Prompt â†’ Model â†’ Parser)**  
The only pattern that survives production. Every single time you ask an LLM for structured data, the chain must end with a real parser. No â€œIâ€™ll just grab the text and hopeâ€ allowed. This is the foundation of every reliable agent in 2025.


**2. Schema-First**  
You write the Pydantic model first â€” before the prompt, before anything. The schema is the source of truth. Then you auto-generate the format instructions from it (via parser.get_format_instructions()) and inject them into the prompt. This single habit eliminates 90% of malformed outputs and hallucinations.

**3. Guardrail Loop (Retry + Feedback)**  
When parsing fails, you do not give up. You take the exact error message, append it to the prompt with a stern correction (â€œYou returned invalid JSON because: missing key â€˜priceâ€™. Return ONLY corrected JSON.â€), and retry â€” up to 3 times max. This loop turns an 85% success rate into 99.8%. Every serious production system uses it.

**4. Native-First**  
Before you write a single line of parsing logic, check: does your model support native JSON mode or tool calling? (GPT-4o, Claude 3.5 Sonnet/Opus, Gemini 1.5 Pro, Grok-4 all do.) If yes â€” use it first. The parser becomes a thin validation layer instead of a heavy repair engine. Speed and reliability both win.

**5. Composable Chain**  
Everything is a typed, reusable chain:  
prompt â†’ llm â†’ parser â†’ next_step  
Each step outputs a Pydantic model. You can log, test, mock, cache, and monitor every arrow independently. This is how real teams build agents that donâ€™t collapse at scale.

---
### 2.3.2 Proven Workflows â€” Extended

**Workflow A â€” Classification**  
User message â†’ classification prompt â†’ LLM in JSON mode â†’ Pydantic parser â†’ routing logic  
Used for: sentiment, toxicity, intent, escalation, spam, language detection

**Workflow B â€” Structured Extraction**  
Raw document â†’ extraction prompt with clear schema â†’ LLM â†’ OutputParser with auto-repair â†’ validated object â†’ save to DB  
Used for: invoices, resumes, forms, contracts, medical records

**Workflow C â€” Agent with Tools**  
User query â†’ LLM in tool-calling mode â†’ structured function call â†’ lightweight parser â†’ execute real function â†’ feed result back  
This is the modern ReAct pattern done correctly.

**Workflow D â€” Structured Summarization**  
Long text â†’ split into chunks â†’ each chunk gets a structured summary prompt â†’ LLM â†’ parser â†’ clean summary objects â†’ final merge  
Prevents hallucinated bullet points and keeps facts aligned.

**Workflow E â€” Multi-Step Data Collection**  
Step 1 prompt â†’ parser â†’ validate â†’ Step 2 prompt (pre-filled with previous answers) â†’ parser â†’ â€¦ â†’ final object  
Used for: forms, onboarding, diagnostics, complex user input gathering

---
### 2.3.3 Anti-Patterns â€” Extended (Never Do These)

**1. Using raw LLM text downstream**  
Treating the model output as a string and doing .split() or .find() later. One â€œSorry, I canâ€™t helpâ€ and your whole pipeline dies.

**2. Regex-only parsing**  
Works until the model adds a space, an emoji, or changes tone. Then it silently breaks forever.

**3. No retry logic**  
Accepting that 10% of outputs will be malformed and just letting them fail. Thatâ€™s not a system â€” thatâ€™s a lottery.

**4. Burying format instructions in the middle of the prompt**  
The model ignores everything after 300 tokens. Format instructions must be at the very end, clearly marked.

**5. Skipping schema validation**  
You get a string â€œ28â€ where you expected an int, then your math breaks silently. Always validate types.

**6. Deeply nested schemas (4+ levels)**  
The deeper the nesting, the higher the failure rate â€” exponentially. Keep schemas flat whenever possible.

Thatâ€™s it. These are the only patterns that actually work at scale in 2025.

Copy, paste, use forever.  
Ready for Section 3: Quick Reference? Just say â€œ3â€. 

# **2.4 Tools, Tips & Debugging â€” Obsidian-Ready Narrative Version**

The modern LLM ecosystem now treats structured output as a first-class requirement rather than a convenience, and the tooling landscape reflects this shift. Although the field looks complex from the outside, in practice only a small set of components matter. The following section describes them not as scattered tools but as **parts of a single system**, the system that ensures your model outputs are trustworthy.

---

## **### 2.4.1 The Tooling Landscape â€” What Actually Matters Today**

Even though dozens of libraries exist, only a few consistently appear across reliable production pipelines.  
In 2025, structured-output systems tend to converge on the following stack:

- **Pydantic v2** remains the backbone for schema definition. Its Rust-powered validation engine is fast, predictable, and expressive, making it suitable for everything from light prototypes to mission-critical systems.

- **LangChainâ€™s OutputParser suite** continues to offer the easiest â€œglue layerâ€ between LLMs and application logic. Among the set, the _PydanticOutputParser_, _JsonOutputParser_, and _RetryOutputParser_ provide the highest reliability with minimal configuration.

- **Native structured-output modes**â€”particularly OpenAIâ€™s JSON mode, Anthropicâ€™s tool-use interface, and Geminiâ€™s MIME-type enforcementâ€”have pushed syntactic error rates close to zero in practical use.

- **LLM repair utilities**, such as json-repair or handcrafted heuristic sanitizers, still play a role when dealing with older models or free-tier model variants that lack strict JSON enforcement.
 
- **Constraint-driven decoding frameworks**, such as Outlines or LM Format Enforcer, offer mathematically guaranteed structure, though often at a cost of slower inference.


Taken together, this small cluster forms the â€œcore toolkitâ€ of any robust structured-output workflow. Everything else tends to be optional.

---

## **### 2.4.2 A Practical Debugging Protocol**

Debugging LLM structured output is less about clever tricks and more about **methodical observation**.

A reliable debugging workflow usually moves through these phases:

1. **Observe the raw model completion**  
    Most structural failures reveal themselves immediatelyâ€”extra commentary, truncated braces, missing fields, or subtle violations like single quotes in strings.
2. **Isolate the failure**  
    Run the same prompt across different temperatures, models, and decoding modes. The contrast often reveals whether the failure is an instruction issue, a model capability issue, or simply a formatting oversight.
3. **Stress-test the schema**  
    Use adversarial examples: emoji-heavy sentences, multilingual inputs, excessively long lists, and malformed symbols. A schema that only works on clean data is not a schemaâ€”itâ€™s a hope.
4. **Apply controlled adjustments**  
    Adjust the promptâ€™s format instructions, reduce temperature, shorten the context, or reinforce a single structural rule rather than rewriting everything.

This simple cycle tends to catch 90% of structured-output issues before they reach production.

---

## **### 2.4.3 Error Patterns and Their Proven Remedies**

LLMs fail in predictable ways, and recognizing these patterns makes recovery much easier.  
Although many errors appear chaotic at first glance, they tend to cluster into a few categories:

- **Trailing commas**, which are still the most common cause of JSON failures.
- **Single-quoted strings**, especially in free or permissive models.
- **Premature explanations**, where the model apologizes or adds context before the JSON block.
- **Missing required fields** or delivering fields with the wrong type.
- **Hallucinated extras**, such as adding â€œadditional_infoâ€ or â€œconfidence_scoreâ€ when none was requested.

Experience shows that the following remediation hierarchy usually produces the highest reliability:

1. Use a **native structured-output mode** whenever available.
2. Keep instructions short and place format instructions **near the end** of the prompt.
3. Apply a **json-repair step** for syntactic cleanup.
4. Enforce a schema with **Pydantic**, rejecting anything not explicitly allowed.
5. Use a **targeted retry parser** that tells the model exactly what failed.

These steps work not because they are clever, but because they align with what LLMs are good at: repetition, correction, and structure reinforcement.

---

## **### 2.4.4 Performance & Reliability Guidelines**

The more complex the structured output becomes, the more brittle the system grows. Real-world experiments across multiple labs converge on a set of practical guidelines that significantly improve reliability:

- **Temperature 0.0** remains essential for structured tasks.
- **Retry limits** beyond three attempts show rapidly diminishing returns.
- **Deeply nested schemas** (more than three or four layers) dramatically increase failure rates.
- **Long lists** cause attention drift; keeping lists under twenty items tends to stabilize results.
- **Format instructions should appear within the last few hundred tokens of the prompt**, ensuring they are fresh in the modelâ€™s short-term attention window.

These are not arbitrary rules; they come from empirical testing across tens of thousands of completions.

---

## **### 2.4.5 Monitoring & Operational Oversight**

A structured-output system is only as good as its ongoing monitoring.  
Healthy systems tend to keep an eye on:

- **Parse success rate**, which should remain above 99.5%.
- **Average retry count**, ideally below 1.2 per call.
- **Validation errors**, which should remain rare unless schema complexity changes.
- **p95 latency**, which grows significantly when repair or retry loops become frequent.

Small deviations in these metrics usually signal changes in upstream model behavior long before user-facing failures appear.